% LaTeX source for ``Think Stats:
% Probability and Statistics for Programmers''
% Copyright 2010  Allen B. Downey.

% License: Creative Commons Attribution-Share Alike 3.0 Unported
% http://creativecommons.org/licenses/by-sa/3.0/
%

%\documentclass[10pt,b5paper]{book}
\documentclass[10pt]{book}
\usepackage[width=5.5in,height=8.5in,
  hmarginratio=3:2,vmarginratio=1:1]{geometry}

% for some of these packages, you might have to install
% texlive-latex-extra (in Ubuntu)

\usepackage{pslatex}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{exercise}                        % texlive-latex-extra
\usepackage{makeidx}
\usepackage{setspace}
\usepackage{hevea}                           
\usepackage{upquote}

\title{Think Stats}
\newcommand{\thetitle}{Think Stats: Probability and Statistics for Programmers}
\newcommand{\theversion}{1.0.0}

% these styles get translated in CSS for the HTML version
\newstyle{a:link}{color:black;}
\newstyle{p+p}{margin-top:1em;margin-bottom:1em}
\newstyle{img}{border:0px}

% change the arrows
\setlinkstext
  {\imgsrc[ALT="Previous"]{back.png}}
  {\imgsrc[ALT="Up"]{up.png}}
  {\imgsrc[ALT="Next"]{next.png}} 

\makeindex

\begin{document}

\frontmatter

% LATEXONLY

\input{latexonly}

\newtheorem{ex}{Exercise}[chapter]

\begin{latexonly}

\renewcommand{\blankpage}{\thispagestyle{empty} \quad \newpage}

%\blankpage
%\blankpage

% TITLE PAGES FOR LATEX VERSION

%-half title--------------------------------------------------
\thispagestyle{empty}

\begin{flushright}
\vspace*{2.0in}

\begin{spacing}{3}
{\huge Think Stats: Probability and Statistics for Programmers}\\
{\Large }
\end{spacing}

\vspace{0.25in}

Version \theversion

\vfill

\end{flushright}

%--verso------------------------------------------------------

\blankpage
\blankpage
%\clearemptydoublepage
%\pagebreak
%\thispagestyle{empty}
%\vspace*{6in}

%--title page--------------------------------------------------
\pagebreak
\thispagestyle{empty}

\begin{flushright}
\vspace*{2.0in}

\begin{spacing}{3}
{\huge Think Stats}\\
{\Large Probability and Statistics for Programmers}
\end{spacing}

\vspace{0.25in}

Version \theversion

\vspace{1in}


{\Large
Allen Downey\\
}


\vspace{0.5in}

{\Large Green Tea Press}

{\small Needham, Massachusetts}

%\includegraphics[width=1in]{figs/logo1.eps}
\vfill

\end{flushright}


%--copyright--------------------------------------------------
\pagebreak
\thispagestyle{empty}

{\small
Copyright \copyright ~2010 Allen Downey.


\vspace{0.2in}

\begin{flushleft}
Green Tea Press       \\
9 Washburn Ave \\
Needham MA 02492
\end{flushleft}

Permission is granted to copy, distribute, and/or modify this document
under the terms of the Creative Commons Attribution-Share Alike 3.0 Unported
License, which is available at \url{creativecommons.org/licenses/by-sa/3.0/}.

The original form of this book is \LaTeX\ source code.  Compiling this
code has the effect of generating a device-independent representation
of a textbook, which can be converted to other formats and printed.

The \LaTeX\ source for this book is available from
\url{www.thinkstats.com}.

The cover for this book is based on a photo by Paul Friel
(\url{flickr.com/people/frielp/}), who made it available under
the Creative Commons Attribution license.  The original photo
is at \url{flickr.com/photos/frielp/11999738/}.

\vspace{0.2in}

} % end small

\end{latexonly}


% HTMLONLY

\begin{htmlonly}

% TITLE PAGE FOR HTML VERSION

{\Large \thetitle}

{\large Allen B. Downey}

Version \theversion

\setcounter{chapter}{-1}

\end{htmlonly}


\chapter{Preface}

\section*{Programming is a pedagogic wedge}



Allen B. Downey \\
Needham MA\\

Allen Downey is an Associate Professor of Computer Science at 
the Franklin W. Olin College of Engineering.




%\section*{Acknowledgements}



\section*{Contributor List}

\index{contributors}

If you have a suggestion or correction, please send email to 
{\tt downey@allendowney.com}.  If I make a change based on your
feedback, I will add you to the contributor list
(unless you ask to be omitted).

If you include at least part of the sentence the
error appears in, that makes it easy for me to search.  Page and
section numbers are fine, too, but not quite as easy to work with.
Thanks!

\small

\begin{itemize}

\item 

% ENDCONTRIB

\end{itemize}

\normalsize

\clearemptydoublepage

% TABLE OF CONTENTS
\begin{latexonly}

\tableofcontents

\clearemptydoublepage

\end{latexonly}

% START THE BOOK
\mainmatter


\chapter{Statistical thinking for programmers}

This book is about turning data into knowledge.  Data is cheap (at
least relatively); knowledge is harder to come by.

I will present three related pieces:

\begin{description}

\item[Probability] is the study of random events.  Most people have an
  intuitive understanding of degrees of probability, which is why we
  can use words like ``probably'' and ``unlikely'' without special
  training, but we will talk about how to make quantitative claims
  about those degrees.

\item[Statistics] is the discipline of using data samples to support
  claims about populations.  Most statistical analysis is based on
  probability, which is why these pieces are usually presented
  together.

\item[Computation] is a tool that is well-suited to quantitative
  analysis, and computers are commonly used to process statistics.
  Also (and more importantly for this book) computational experiments
  are useful for exploring concepts in probability and statistics.

\end{description}

The thesis of this book is that if you know how to program, you can
use that skill to help you understand probability and statistics.
These topics are often presented from a mathematical perspective, and
that approach works well for some people.  But some important ideas
in this area are hard to work with mathematically and relatively
easy to approach computationally.

Both approaches have merits, and the ideal might combine both, but
the goal of this book is to explore the computational path.

The rest of this chapter presents a case study motivated by a question
I heard when my wife and I were expecting our first child: do first
babies tend to arrive late?

\section{Do first babies arrive late?}

If you Google this question, you will find plenty of discussion.
Some people claim it's true, others say it's a myth, and some people
say it's the other way around: first babies come early.

In many of these discussions, people provide data to support their
claims.  I found many examples like these:

\begin{quote}

``My two friends that have given birth recently to their first babies,
BOTH went almost 2 weeks overdue before going into labour or being
induced.''

``My first one came 2 weeks late and now I think the second one is
going to come out two weeks early!!''

``I don't think that can be true because my sister was my mother's
first and she was early, as with many of my cousins.''

\end{quote}

Reports like these are called {\bf anecdotal evidence} because they
are based on data that is unpublished and usually personal.  In casual
conversation, there is nothing wrong with anecdotes, so I don't mean
to pick on the people I quoted.

But we might want evidence that is more persuasive and
an answer that is more reliable.  By those standards, anecdotal
evidence usually fails, because:

\begin{description}

\item[Small number of observations:] If the gestation period is longer
  for first babies, the difference is probably small compared to the
  natural variation.  In that case, we might have to compare a large
  number of pregnancies to be sure there is really a difference (or
  not).

\item[Selection bias:] People who join a discussion of this question
  might be interested because their first babies were late.  In that
  case the process of selecting data would bias the results.

\item[Confirmation bias:] People who believe the claim might be more
  likely to contribute examples that confirm it.  People who doubt the
  claim are more likely to cite counterexamples.

\item[Inaccuracy:] Anecotes are often personal stories that are
  (deliberately or not) misremembered, misrepresented, repeated
  inaccurately, etc.

\end{description}

So how can we do better?

\section{A statistical approach}

To address the limitations of anecdotes, we will use the tools
of statistics, which include:

\begin{description}

\item[Data collection:] We will use data from a large national survey
that was designed explicitly with the goal of generating statistically
valid inferences about the U.S. population.

\item[Exploratory data analysis:] We will start by exploring the dataset
to get a sense of what questions were asked, what form the answers
are in, and what limitations we might have to address.

\item[Descriptive statistics:] We will generate statistics that summarize
large datasets concisely.

\item[Hypothesis testing:] Where we see apparently effects (like a
difference between two groups), we will evaluate whether the effect
is likely to be real, or whether it might have happened by chance.

\item[Estimation:] We will use measurements in the dataset to estimate
characteristics of the general population.

\end{description}

By performing these steps with care to avoid common pitfalls, we can
reach conclusions that are more justifiable and more likely to be
correct.


\section{The National Survey of Family Growth}

Since 1973 the U.S. Centers for Disease Control and Prevention (CDC)
have conducted the National Survey of Family Growth (NSFG),
which is intended to gather ``information on family life, marriage and
divorce, pregnancy, infertility, use of contraception, and men's and
women's health. The survey results are used ... to plan health services and
health education programs, and to do statistical studies of families,
fertility, and health.''\footnote{See
  \url{cdc.gov/nchs/nsfg.htm}.}

We will use data collected by this survey to investigate whether first
babies tend to come late, and other questions.  In order to use this
data effectively, we have to understand the design of the study.

The NSFG is a {\bf cross-sectional} study, which means that it
captures a snapshot of a group at a point in time.  The most
common alternative is a {\bf longitudinal} study, which observes a
group repeatedly over a period of time.

The NSFG has been conducted seven times; each deployment is called
a {\bf cycle}.  We will be using data from Cycle 6, which was
conducted from January 2002 to March 2003.

The goal of the survey is to draw conclusions about a
{\bf population}; the target population of the NSFG is people in
the United States aged 15-44.

The people who participate in a survey are called {\bf respondents}.
In general, cross-sectional studies are meant to be {\bf
  representative}, which means that every member of the target
population has an equal chance of participating.  Of course that ideal
is hard to achieve in practice, but people who conduct surveys come as
close as they can.

The NSFG not representative; instead it is deliberately {\bf
  oversampled}.  The designers of the study recruited three
groups---Hispanics, African-Americans and teenagers---at rates higher
than their representation in the U.S. population.
The reason for oversampling is to make sure that the number of
respondents in each of these groups is large enough to draw valid
statistical inferences.

Of course, the drawback of oversampling is that it is not as easy
to draw conclusions about the general population based on statistics
from the survey.  We will come back to this point later.

\begin{ex}

Although the NSFG has been conducted seven times, it is not a
longitudinal study.  Read the Wikipedia pages
\url{wikipedia.org/wiki/Cross-sectional_study}
and
\url{wikipedia.org/wiki/Longitudinal_study}
to make sure you understand why not.

\end{ex}

\begin{ex}

In this exercise, you will download data from the NSFG and do some
exploratory data analysis.

\begin{enumerate}

\item Go to \url{thinkstatsbook.com/nsfg.html}.  Read the terms of
use for this data and click ``I accept these terms'' (assuming that you do).

\item Download the files named {\tt 2002FemResp.dat.gz} and {\tt
  2002FemPreg.dat.gz}.  The first is the respondent file, which contains
  one line for each of the 7,643 female respondents.
  The second file contains one line for each pregnancy reported by a
  respondent.

\item Online documentation of the survey is at
  \url{nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm}.
  Browse the sections in the left navigation bar to get a sense of
  what data are included.  You can also read the questionnaires
  at \url{cdc.gov/nchs/data/nsfg/nsfg_2002_questionnaires.htm}.

\item The web page for this book provides code to process the data
  files from the NSFG.  Download \url{thinkstatsbook.com/survey.py}
  and run it in the same directory you put the data files in.  It
  should read the data files and print the number of lines in each:

\begin{verbatim}
Number of respondents 7643
Number of pregnancies 13593
\end{verbatim}

\item Browse the code to get a sense of what it does.  The next
section explains how it works.

\end{enumerate}

\end{ex}

\section{Tables and records}

The poet-philosopher Steve Martin once said:

\begin{quote}
``Oeuf'' means egg, ``chapeau'' means hat.  It's like those French
  have a different word for everything.
\end{quote}

Like the French, database programmers speak a slightly
different language, and since we're working with a database we need
to learn some vocabulary.

Each line in the respondents file contains information about one
respondent.  This information is called a {\bf record}.  The
variables that make up a record are called {\bf fields}.  A
collection of records is called a {\bf table}.

If you read {\tt survey.py} you will see class definitions for {\tt
  Record}, which is an object that represents a record, and {\tt
  Table}, which represents a table.

There are two subclasses of
{\tt Record}, {\tt Respondent} and {\tt Pregnancy}, which will
contain records from the respondent and pregnancy tables.
For the time being, these classes are empty; in particular, there
is no {\tt init} method to initialize their attributes.  Instead
we will use {\tt Table.MakeRecord} to convert a line of text into
a {\tt Record} object.

There are also two subclasses of {\tt Table}, {\tt Respondents}
and {\tt Pregnancies}.  The {\tt init} method in each class
specifies the default name of the data file and the type of
record to create.  Each {\tt Table} object has an attribute
named {\tt records}, which is a list of {\tt Record} objects.

For each {\tt Table}, the {\tt GetFields} method returns
a list of tuples that specify the fields from the record that
will be stored as attributes in each {\tt Record} object.

For example, here is {\tt Pregnancies.GetFields}:

\begin{verbatim}
    def GetFields(self):
        return [
            ('caseid', 1, 12, int),
            ('prglength', 275, 276, int),
            ('outcome', 277, 277, int),
            ('birthord', 278, 279, int),
            ('finalwgt', 423, 440, float),
            ]
\end{verbatim}

The elements of these tuples are:

\begin{description}

\item[field:] The name of the attribute where the field
will be stored.  Most of the time I use the names from the
NSFG codebook, converted to all lower case.

\item[start:] The index of the starting location for this
field.  You can look up these indices in the NSFG codebook
at \url{nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm}.
For example, the indices for {\tt caseid} are
1--12.

\item[end:] The index of the ending location for this
field.

\item[conversion function:] A function that takes a string
and converts it to an appropriate type.  You can use built-in
functions like {\tt int} and {\tt float} or a user-defined
function.  If the conversion fails, the attribute gets the
string value {\tt 'NA'}.  If you don't want to convert a
field, you can provide an identity function or use {\tt str}.

\end{description}

For pregnancy records, we extract the following variables:

\begin{description}

\item[caseid] is the integer ID of the associated respondent.

\item[prglength] is the integer duration of the pregnancy in weeks.

\item[outcome] is an integer code for the outcome of the pregnancy.
The code 1 indicates a live birth.

\item[birthord] is the integer birth order of each live birth;
for example, the code for a first child is 1. 
For outcomes other than live birth, this field is blank.

\item[finalwgt] is the statistical weight associated with the respondent.
It is a floating-point value that indicates the number of people in
the U.S. population this respondent represents.  Members of undersampled
groups have higher weights.

\end{description}

If you read the casebook carefully, you will see that most of these
variables are {\bf recodes}, which means that they are not part
of the {\bf raw data} collected by the survey, but they are
calculated using the raw data.

For example, {\tt prglength} for live births is equal to the raw
variable {\tt wksgest} (weeks of gestation) if it is available;
otherwise it is estimated using {\tt mosgest * 4.33} (months of
gestation times the average number of weeks in a month).

Recodes are often based on logic that checks the consistency and
accuracy of the data.  In general it is a good idea to use recodes
unless there is a compelling reason to process the raw data
yourself.

\begin{ex}

In this exercise you will write a program to explore the data
in the Pregnancies table.

\begin{enumerate}

\item In the directory where you put {\tt survey.py} and the
data files, create a file named \verb"first_baby.py" and
type or paste in the following code:

\begin{verbatim}
import survey
preg_table = survey.Pregnancies()
print 'Number of pregnancies', len(preg_table.records)
\end{verbatim}

The result should be 13593 pregnancies.

\item Write a loop that iterates \verb"preg_table" and counts
the number of live births.  Find the documentation of {\tt outcome}
and confirm that your result is consistent with the summary
in the documentation.

\item Modify the loop to partition the live birth records into
two groups, one for first babies and one for the others.  Again,
read the documentation of {\tt birthord} to see if your results
are consistent.

When you are working with a new dataset, these kinds of checks
are useful for finding errors and inconsistencies in the data,
detecting bugs in your program, and checking your understanding
of the way the data are encoded.

\item Compute the average pregnancy length (in weeks) for first
babies and others.  Is there a difference between the groups?  How
big is it?

Note: For now we are ignoring the {\tt finalwgt} variable.

\end{enumerate}

You can download a solution to this exercise from
\url{thinkstatsbook.com/survey.py}.

\end{ex}

\section{Significance}

In the previous exercise, you compared the gestation
period for first babies and others, and if things worked out,
you found that first babies are born about 13 hours later,
on average.

A difference like that is called an {\bf apparent effect};
that is, there seems to be something going on, but we are not
yet sure.  There are several questions we still want to ask:

\begin{itemize}

\item If the two groups have different means, what about other {\bf
  summary statistics}, like median and variance?  Can we be more
  precise about how the groups differ?

\item Is it possible that the difference we saw could occur by chance,
  even if the groups we compared were actually the same?  If so,
  we would conclude that the effect was not {\bf statistically
    significant}.

\item Is it possible that the apparent effect is due to sample bias or
  some other error in the experimental setup?  If so, then we might
  conclude that the effect is an {\bf artefact}; that is, something we
  created (by accident) rather than found. 

\end{itemize}

We will address these questions in the next few chapters.

\section{Glossary}

\begin{description}

\item[anecdotal evidence:] Evidence, often personal, that is collected
  casually rather by a well-designed study.

\item[population:] A group we are interested in studying, often a
  group of people, but the term is also used for animals, vegetables
  and minerals\footnote{If you don't recognize this phrase, see
    \url{wikipedia.org/wiki/Twenty_Questions}.}.

\item[cross-sectional study:] A study that collects data about a
population at a particular point in time.

\item[longitudinal study:] A study that follows a population over
time, collecting data from the same group repeatedly.

\item[respondent:] A person who responds to a survey.

\item[sample:] The subset of a population used to collect data.

\item[representative:] A sample is representative if every member
of the population has the same chance of being in the sample.

\item[oversampling:] The technique of increasing the representation
of a sub-population in order to avoid errors due to small sample
sizes.

\item[record:] In a database, a collection of information about
a single person or other object of study.

\item[field:] In a database, one of the named variables that makes
up a record.

\item[table:] In a database, a collection of records.

\item[raw data:] Values collected and recorded with little or no
checking, calculation or interpretation.

\item[recode:] A value that is generated by calculation and other
logic applied to raw data.

\item[summary statistic:] The result of a computation that reduces
a dataset to a single number (or at least a smaller set of numbers)
that captures some characteristic of the dataset.

\item[apparent effect:] A measurement or summary statistic that
suggests that something interesting is happening.

\item[statistically significant:] An apparent effect is statistically
  significant if it is unlikely to occur by chance.

\item[artefact:] An apparent effect that is caused by bias,
  measurement error, or some other kind of error.

\end{description}

\section{Exercises}

\begin{ex}

The best way to learn about statistics is to work on a project you are
interested in.  Is there a question like, ``Do first babies arrive
late,'' that you would like to investigate?

Think about questions you find personally interesting, or items of
conventional wisdom, or controversial topics, or questions that have
political consequences, and see if you can formulate a question that
lends itself to statistical inquiry.

Now start looking for datasets to help you address the
question.  Governments are good sources because data from
public research is often freely available\footnote{On the day
I wrote this paragraph, a court in the UK ruled that the
Freedom of Information Act applies to scientific research data.}.

Another way to find data is Wolfram Alpha, which is a curated
collection of good-quality datasets at \url{wolframalpha.com}.
Results from Wolfram Alpha are subject to copyright
restrictions; you might want to check the terms before you commit
yourself.

Google and other search engines can also help you find data, but it
can be harder to evaluate the quality of resources on the web.

If it seems like someone has answered your question, look closely to
see whether the answer is justified.  There might be flaws in the data
or the analysis that make the conclusion unreliable.  In that case you
could perform a different analysis of the same dataset, or look for a
better source of data.

If you find a published paper that addresses your question, you
should be able to get the raw data.  Many authors make their data
available on the web, but for sensitive data you might have to
write to the authors, provide information about how you plan to use
the data, or agree to certain terms of use.  Be persistent!

\end{ex}


\chapter{Descriptive statistics}

Distributions

pmf
cdf

percentiles

moments

modes

   multimodal distributions (artefact examples: heights, gestation)

\printindex

\clearemptydoublepage
%\blankpage
%\blankpage
%\blankpage


\end{document}
